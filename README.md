# ğŸ–¼ï¸ AI vs Human Image Classification â€“ Vision Transformer Project

Can machine learning detect whatâ€™s *real* and whatâ€™s AI-generated? In this project, I fine-tuned **Googleâ€™s Vision Transformer (ViT)** using the Hugging Face ecosystem to classify images as either **AI-generated** or **real**.

The goal: build a model that can distinguish human-taken photos from AI-generated visuals with high accuracyâ€”even when the differences are subtle.

---

## ğŸ“Š Overview

- **Type:** Binary Image Classification  
- **Model:** Vision Transformer (ViT â€“ `google/vit-base-patch16-224-in21k`)  
- **Frameworks:** Hugging Face Transformers, PyTorch, TensorFlow, scikit-learn  
- **Dataset Size:** 11GB+ (custom blend of AI and real images)  
- **Tools:** ğŸ¤— Datasets, Transformers, Matplotlib, scikit-learn  

---

## ğŸ”§ Project Breakdown

### ğŸ“ 1. Dataset & Preprocessing
- Compiled and labeled a dataset of **real** vs **AI-generated** images
- Applied normalization, resizing, and augmentation (horizontal flip, rotation, brightness adjustment) using `torchvision.transforms`
- Split dataset: 70% train, 15% validation, 15% test

### ğŸ§  2. Model & Training
- Fine-tuned Googleâ€™s pre-trained **ViT model** using Hugging Face Trainer API  
- Used **AdamW optimizer**, **cross-entropy loss**, and early stopping  
- Trained for 3 epochs on GPU, monitoring training loss and validation loss

### ğŸ“ˆ 3. Results:
- Minimal overfitting thanks to data augmentation and early stopping

---

## ğŸ“ Want More Detail?

I wrote a blog walking through the full process:
ğŸ”— [Read it here â†’](https://pranjalikdeshmukh1.wixsite.com/my-site-1/post/classifying-ai-generated-images-using-google-s-vision-transformer-vit)

---

## ğŸ§ª Try It Yourself

```bash
git clone https://github.com/PranjaliD11/AI_Vs_Human_Classifier.git
cd AI_Vs_Human_Classifier
open classifier_notebook.ipynb
